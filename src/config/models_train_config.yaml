1.3b:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 1
  block_size: 2048
  d_heads: 64
  d_model: 2048
  dropout_prob: 0.1
  global_gradient_norm: 1.0
  learning_rate_decay: 0.1
  max_learning_rate: 2.0e-04
  n_heads: 32
  n_layers: 24
  vocab_size: 50066
  warmup_steps: 500
  weight_decay: 0.1
125m:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 500000
  block_size: 2048
  d_heads: 64
  d_model: 768
  dropout_prob: 0.1
  eval_step: 256
  global_gradient_norm: 1.0
  learning_rate_decay: 0.1
  max_learning_rate: 6.0e-4
  n_heads: 12
  n_layers: 12
  vocab_size: 50066
  warmup_steps: 500
  weight_decay: 0.1
6.7b:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 2000000
  block_size: 2048
  d_heads: 128
  d_model: 4096
  dropout_prob: 0.1
  global_gradient_norm: 1.0
  learning_rate_decay: 0.1
  max_learning_rate: 1.2e-04
  n_heads: 32
  n_layers: 32
  vocab_size: 50000
  warmup_steps: 500
  weight_decay: 0.1
small_opt:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 2
  block_size: 2048
  dropout_prob: 0.1
  ffn_dim: 16
  global_gradient_norm: 1.0
  hidden_size: 16
  learning_rate_decay: 0.1
  max_learning_rate: 2.0e-04
  max_position_embeddings: 2048
  num_attention_heads: 1
  num_hidden_layers: 1
  vocab_size: 50000
  warmup_steps: 500
  weight_decay: 0.1
  word_embed_proj_dim: 16
mistral7b:
  vocab_size: 32000
  block_size: 2048
  hidden_size: 4096
  intermediate_size: 14336
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 8
  hidden_act: 'silu'
  max_position_embeddings: 131072
  initializer_range: 0.02
  rms_norm_eps: 1.0e-6
  use_cache: True
  pad_token_id: None
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: False
  rope_theta: 10000.0
  sliding_windows: 1024
  global_gradient_norm: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
llama2:
  "_name_or_path": "meta-llama/Llama-2-7b-hf"
  architectures: ["LlamaForCausalLM"]
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: "silu"
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 4096
  model_type: "llama"
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: null
  tie_word_embeddings: false
  torch_dtype: "float16"
  transformers_version: "4.31.0.dev0"
  use_cache: true
  vocab_size: 32000
  dim: 4096
  block_size: 2048
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  warmup_steps: 2000
