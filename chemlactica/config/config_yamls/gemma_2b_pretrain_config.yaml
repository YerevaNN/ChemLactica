train_config:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 500000
  dropout_prob: 0.1
  eval_step: 256
  global_gradient_norm: 1.0
  learning_rate_decay: 0.1
  max_learning_rate: 5.0e-5
  n_heads: 12
  n_layers: 18
  warmup_steps: 500
  weight_decay: 0.1
model_config:
  block_size: 2048
  vocab_size: 256000
  separator_token: <bos>
  tokenizer_path: "chemlactica/tokenizer/GemmaTokenizer"
