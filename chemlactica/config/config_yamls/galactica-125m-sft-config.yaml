train_config:
  adam_beta1: 0.9
  adam_beta2: 0.95
  batch_size: 500000
  dropout_prob: 0.1
  eval_step: 256
  global_gradient_norm: 1.0
  learning_rate_decay: 0.1
  max_learning_rate: 1.0e-4
  warmup_steps: 0
  weight_decay: 0.1
  bf16: true
  bf16_full_eval: true
  fp16: false
  tf32: true
  evaluation_strategy: "steps"
  save_total_limit: 4
  grad_accumulation_scheduler: false
  dynamic_grad_accumulation: false
  grad_accumulation_patience: 4000
  grad_accumulation_max: 256
  grad_accumulation_delta_steps: 100
  grad_accumulation_delta_percentage: 0.02
model_config:
  n_heads: 12
  n_layers: 12
  block_size: 2048
  vocab_size: 50000
  separator_token: </s>
  separator_token_id: 2
  tokenizer_path: "facebook/galactica-125m"
